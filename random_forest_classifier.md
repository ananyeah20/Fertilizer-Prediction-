## RANDOM FOREST CLASSIFIER

Random forest is a machine learning algorithm based on ensemble learning. It can be used both as a regressor as well as a classifier. Random Forest combines multiple  decision trees generated by creating decision trees on randomly selected data, to form a forest of trees, considers the average from each decision tree which avoids biases and also handles the missing values thereby overcoming the overfitting issues. The 
algorithm isnâ€™t affected on introducing new data in the 
dataset and in our model it works well because the 
 dataset contains both categorial and numeric data. The 
  only disadvantage being it takes more time to train the 
          data, compared to other algorithms ,due to its 
          complexity. Random Forest indicates the significance of 
each feature which is calculated using Gini importance 
i.e. the mean decrease in the impurity. This estimates the accuracy which decreases when a variable is dropped.
The variable is highly significant to fit if the decrease is large. This mean decrease estimated influences the variable selection.

Though Random Forest is the oldest classifier in use yet it is proved to be more accurate, easy and faster to implement.This classifier is effective on large datasets and provides higher precision.It usually gives an accuracy of 96.6%. It gives better performance in terms of accuracy compared to other classifiers such as SVM. In comparison to neural networks, random forest does not require a GPU to train the data. In some classifiers such as QdaCov_t, lda_R the accuracy is affected by the collinearity of data.
Many other such as rrlda_R show errors due to discrete inputs for all the training patterns.